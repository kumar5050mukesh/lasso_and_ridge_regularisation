{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "# represent?\n",
    "\"\"\"R-squared is a statistical measure that represents the goodness of fit of a linear regression model. \n",
    "\n",
    "\n",
    "The R-squared value ranges from 0 to 1, where 0 indicates least goodness of fit and 1 indicates the most goodness fit.\n",
    "\n",
    "R-squared is calculated as the ratio of the sum of squared differences between the predicted and actual \n",
    "values to the total sum of squared differences between the dependent variable and its mean. \n",
    "R-squared = 1 - (sum of squared residuals / total sum of squares)\n",
    "\n",
    "An r squared value of 0.80 indicates that 80 percent of the dependent variable can be explained by the \n",
    "independent variable \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\"\"\"Adjusted r squared is modified version of r squared that takes in more than one independent variable.\n",
    "while r squared measures the proportion of variance in the dependent variable that is explained by the independent \n",
    "variable , adjusted r squared penalises the addition of independent variable\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - p - 1)]\n",
    "\n",
    "where n is the sample size and p is the number of independent variables in the model.\n",
    "\n",
    "The formula for adjusted R-squared incorporates a penalty factor that adjusts for the number of independent variables \n",
    "in the model. As the number of independent variables increases, the penalty factor increases, resulting in a lower \n",
    "adjusted R-squared value than the regular R-squared value.\n",
    "\n",
    " \n",
    "It accounts for the potential overfitting of the model due to the inclusion of irrelevant independent variables \n",
    "\n",
    "\n",
    " adjusted R-squared is a modified version of R-squared that accounts for the number of independent \n",
    "variables in the linear regression model. It penalizes the inclusion of irrelevant independent variables and \n",
    " a more conservative measure of the model's goodness of fit than R-squared.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\"\"\"Adjusted r squared is more appropriate than r squared when evaluating the goodness of fit of linear regression model \n",
    "with multiple independent variables .This is because r squared may falsely give high estimate of the model performance.\n",
    "\n",
    " It is more appropriate to use adjusted R-squared when comparing the goodness of fit of different linear regression\n",
    " models with different numbers of independent variables. Adjusted R-squared takes into account the number of independent \n",
    " variables in the model and provides a more conservative estimate of the model's goodness of fit. This helps to avoid \n",
    " overfitting of the model, which occurs when additional independent variables are added that do not significantly contribute\n",
    "   to the model's explanatory power.\n",
    "\n",
    "\n",
    "adjusted R-squared is more appropriate than R-squared when evaluating the goodness of fit of a linear \n",
    "regression model with multiple independent variables, comparing the goodness of fit of different models, and determining \n",
    "the significance of independent variables in predicting the dependent variable.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "# calculated, and what do they represent?\n",
    "\"\"\"In the context of regression analysis,root mean square error , mean square error , and mean absolute error  are metrics\n",
    " used to evaluate the accuracy of a regression model's \n",
    " \n",
    "Root Mean Squared Error\n",
    "RMSE is the square root of the mean of the squared differences between the predicted and actual values. It is calculated as follows:\n",
    "RMSE = sqrt(mean((predicted - actual)^2))\n",
    "\n",
    "\n",
    "\n",
    "RMSE is a measure of the average magnitude of the error in the model's predictions.\n",
    " A lower RMSE value indicates better accuracy of the model's predictions.\n",
    "\n",
    "Mean Squared Error \n",
    "MSE is the mean of the squared differences between the predicted and actual values. It is calculated as follows:\n",
    "MSE = mean((predicted - actual)^2)\n",
    "\n",
    "\n",
    "\n",
    "MSE is a measure of the average squared error in the model's predictions. \n",
    "A lower MSE value indicates better accuracy of the model's predictions.\n",
    "\n",
    "Mean Absolute Error \n",
    "MAE is the mean of the absolute differences between the predicted and actual values. It is calculated as follows:\n",
    "MAE = mean(abs(predicted - actual))\n",
    "\n",
    "\n",
    "MAE is a measure of the average magnitude of the error in the model's predictions, but unlike RMSE, \n",
    "it does not give more weight to larger errors. A lower MAE value indicates better accuracy of the model's predictions.\n",
    "\n",
    "root mean square error is used when there is no outliers in the data because root mean square punishes the outliers\n",
    "on the other hand the mean squared error and the mean absolute error is used when the data has outliers.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "# regression analysis.\n",
    "\"\"\"\n",
    "\n",
    "Advantages \n",
    "(1)easy to understand and calculate\n",
    "(2)standardized measure of the error between predicted and actual values\n",
    "(3)give more weight to larger errors, which can be useful when large errors are particularly problematic.\n",
    "(4)MAE is more robust to outliers compared to RMSE and MSE, as it only takes the absolute value of the error.\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "(1)whether the model overestimates or underestimates the actual value\n",
    "(2)assume that the errors are normally distributed and have constant variance\n",
    "(3)do not provide information about the goodness of fit of the model beyond the accuracy of its predictions,\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "# it more appropriate to use?\n",
    "\"\"\"Lasso regularization is a method used in machine learning and statistics to prevent overfitting of a model by adding \n",
    "a penalty term to the loss function. \n",
    "\n",
    "The lasso regularization shrinks the coefficients towards zero, which can lead to some of the coefficients becoming \n",
    "exactly zero, resulting in feature selection. This makes lasso regularization a useful tool for feature selection \n",
    "when there are many features, some of which are not important.\n",
    "\n",
    "In contrast, Ridge regularization shrinks the coefficients towards zero as well, but the penalty term is based \n",
    " the squared values of the coefficients instead of the absolute values. This leads to small but non-zero values \n",
    " for all coefficients, rather than selecting some coefficients and setting others to exactly zero. Ridge regularization \n",
    " is more appropriate when all features are expected to have some impact on the outcome.\n",
    "\n",
    "When to use lasso regularization or ridge regularization depends on the specific problem and the data at hand. \n",
    "If there are many features in the dataset and some of them are not relevant, lasso regularization may be more\n",
    " appropriate to perform feature selection. On the other hand, if all features are expected to have some impact \n",
    " on the outcome, ridge regularization may be more appropriate.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "# example to illustrate.\n",
    "\"\"\"Regularized linear models add a penalty term to the loss function that encourages the model to select only the most \n",
    "important features and prevent overfitting. \n",
    "\n",
    "\n",
    "\n",
    "Ridge regression adds a penalty term to the loss function based on the square of the magnitude of the coefficients, \n",
    "while Lasso regression adds a penalty term based on the absolute value of the coefficients. Both methods penalize \n",
    " coefficients, effectively shrinking them towards zero. This helps to reduce the complexity of the model and prevent overfitting.\n",
    "\n",
    " let's say we have a dataset with 10 features, and we fit a linear regression model using all the features. \n",
    "We observe that the model has a high variance, meaning it is overfitting the data. We can apply Lasso or Ridge \n",
    " to reduce the complexity of the model and prevent overfitting. The regularization parameter in Lasso and Ridge regression \n",
    "   the amount of regularization applied to the model. By increasing the regularization parameter, we can further reduce \n",
    "     complexity of the model and prevent overfitting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "# choice for regression analysis.\n",
    "\"\"\" certain limitations that may make them unsuitable for certain types of regression analysis\n",
    "\n",
    "Limited interpretability-----> often produce small coefficients which makes their interpretation unsustainable .\n",
    "\n",
    "Limited flexibility----> regularised linear models often make the the relationship linear. Sometimes the relationship \n",
    "may not be linear.\n",
    "\n",
    "Sensitive to outliers----->regularised linear model are sensitive  to outliers .Outliers can have significant impact on \n",
    "magnitude and direction of coefficients.\n",
    "\n",
    "Bias-Variance tradeoff----->regularised linear model has a trade off between the bias and the variance.\n",
    "\n",
    "Computational complexity--->regularised linear model is difficult to train.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "# Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "# performer, and why? Are there any limitations to your choice of metric?\n",
    "\"\"\"\n",
    "\n",
    "The choice of metric depends on the problem you are trying to solve. RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error)\n",
    " are both used to evaluate regression models. RMSE is more sensitive to outliers than MAE. If you have a few outliers in your data,\n",
    "   RMSE will be higher than MAE. If you don’t have any outliers in your data, RMSE and MAE will be the same.\n",
    "\n",
    "In this case, Model B has a lower MAE than Model A’s RMSE. This means that Model B has a smaller average error than Model A.\n",
    " Therefore, Model B is the better performer.\n",
    "\n",
    "However, there are limitations to using MAE as an evaluation metric. For example, it treats all errors equally and does \n",
    "not differentiate between overestimation and underestimation errors. In contrast, RMSE penalizes larger errors more heavily \n",
    "than smaller errors. Therefore, if you want to penalize larger errors more heavily than smaller errors, RMSE would be \n",
    " better choice of metric\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "# regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "# uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "# better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "# method?\n",
    "\n",
    "\"\"\"The choice of regularization method depends on the problem you are trying to solve. Ridge and Lasso are both used to prevent\n",
    " overfitting in linear regression models. Ridge regression adds a penalty term to the sum of squared errors  that is \n",
    " proportional to the square of the magnitude of the coefficients. Lasso regression adds a penalty term to the SSE that is \n",
    " proportional to the absolute value of the coefficients.\n",
    "\n",
    "In this case, Model A has a smaller regularization parameter than Model B. This means that Model A will have less regularization \n",
    "than Model B. If you have a lot of features in your data, Lasso may be better than Ridge because it can perform feature\n",
    " selection by setting some coefficients to zero. However, if we have only a few features in our  data, Ridge may be better\n",
    "   than Lasso because it does not perform feature selection.\n",
    "\n",
    "Therefore, if we  have a lot of features in your data and want to perform feature selection, Model B would be a better choice. \n",
    "However, if you have only a few features in your data and do not want to perform feature selection, Model A would be a better choice.\n",
    "\n",
    "There are trade-offs and limitations to using regularization methods. For example, Ridge regression can shrink all \n",
    "coefficients towards zero but cannot set any coefficients exactly equal to zero. In contrast, Lasso regression can \n",
    "set some coefficients exactly equal to zero but cannot shrink all coefficients towards zero. Therefore, if you want \n",
    "to perform feature selection and shrink all coefficients towards zero, Elastic Net regression may be a better choice \n",
    "than Ridge or Lasso.\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
